é¡¹ç›® 'cursor-2api' çš„ç»“æ„æ ‘:
ğŸ“‚ cursor-2api/
    ğŸ“„ .env
    ğŸ“„ .env.example
    ğŸ“„ Dockerfile
    ğŸ“„ docker-compose.yml
    ğŸ“„ main.py
    ğŸ“„ nginx.conf
    ğŸ“„ requirements.txt
    ğŸ“‚ app/
        ğŸ“‚ core/
            ğŸ“„ __init__.py
            ğŸ“„ config.py
        ğŸ“‚ providers/
            ğŸ“„ __init__.py
            ğŸ“„ base_provider.py
            ğŸ“„ cursor_provider.py
            ğŸ“„ fetch_override.js
        ğŸ“‚ utils/
            ğŸ“„ sse_utils.py
================================================================================

--- æ–‡ä»¶è·¯å¾„: .env ---

# ====================================================================
# cursor-2api é…ç½®æ–‡ä»¶ (v3.0 - è‡ªä¸»ä»¤ç‰Œç”Ÿæˆç‰ˆ)
# ====================================================================

# --- æ ¸å¿ƒå®‰å…¨é…ç½® (æ¨èè®¾ç½®) ---
# ç”¨äºä¿æŠ¤æ‚¨ API æœåŠ¡çš„è®¿é—®å¯†é’¥ã€‚ç•™ç©ºæˆ–è®¾ä¸º "1" è¡¨ç¤ºä¸è®¾é˜²ã€‚
API_MASTER_KEY=1

# --- éƒ¨ç½²é…ç½® (å¯é€‰) ---
# Nginx å¯¹å¤–æš´éœ²çš„ç«¯å£
NGINX_PORT=8088


--- æ–‡ä»¶è·¯å¾„: .env.example ---

# ====================================================================
# cursor-2api é…ç½®æ–‡ä»¶æ¨¡æ¿ (v2.0 - Cookie è®¤è¯ç‰ˆ)
# ====================================================================

# --- æ ¸å¿ƒå®‰å…¨é…ç½® (æ¨èè®¾ç½®) ---
# ç”¨äºä¿æŠ¤æ‚¨ API æœåŠ¡çš„è®¿é—®å¯†é’¥ã€‚
API_MASTER_KEY=sk-cursor-default-key

# --- Cursor å‡­è¯ (å¿…é¡»è®¾ç½®) ---
# è¯·ç™»å½• cursor.comï¼Œç„¶åä»æµè§ˆå™¨å¼€å‘è€…å·¥å…·çš„ç½‘ç»œ(Network)é¢æ¿ä¸­ï¼Œ
# æ‰¾åˆ°å¯¹ /api/chat çš„è¯·æ±‚ï¼Œå¹¶å¤åˆ¶å…¶è¯·æ±‚å¤´(Request Headers)ä¸­çš„å®Œæ•´ "cookie" å­—ç¬¦ä¸²ã€‚
CURSOR_COOKIE="åœ¨æ­¤å¤„ç²˜è´´æ‚¨çš„å®Œæ•´ Cookie å­—ç¬¦ä¸²"

# --- éƒ¨ç½²é…ç½® (å¯é€‰) ---
# Nginx å¯¹å¤–æš´éœ²çš„ç«¯å£
NGINX_PORT=8088


--- æ–‡ä»¶è·¯å¾„: Dockerfile ---

# ====================================================================
# Dockerfile for cursor-2api (v3.1 - Definitive Edition)
# ====================================================================

FROM python:3.10-slim

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1
WORKDIR /app

# --- 1. å®‰è£… Python ä¾èµ– ---
# é¦–å…ˆåªå¤åˆ¶ requirements.txt ä»¥ä¾¿åˆ©ç”¨ Docker çš„æ„å»ºç¼“å­˜ã€‚
# åªæœ‰å½“ requirements.txt æ–‡ä»¶æ”¹å˜æ—¶ï¼Œè¿™ä¸€å±‚æ‰ä¼šé‡æ–°æ„å»ºã€‚
COPY requirements.txt .
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt

# --- 2. å®‰è£… Playwright çš„ç³»ç»Ÿçº§ä¾èµ– (ä»¥ root èº«ä»½) ---
# ä½¿ç”¨ python -m playwright æ¥ç¡®ä¿å‘½ä»¤å¯ä»¥è¢«æ‰¾åˆ°ï¼Œè¿™æ˜¯æœ€ç¨³å¥çš„æ–¹å¼ã€‚
RUN python -m playwright install-deps

# --- 3. å¤åˆ¶æ‰€æœ‰åº”ç”¨ä»£ç  ---
COPY . .

# --- 4. åˆ›å»ºå¹¶åˆ‡æ¢åˆ°é root ç”¨æˆ· ---
RUN useradd --create-home appuser && \
    chown -R appuser:appuser /app
USER appuser

# --- 5. å®‰è£… Playwright æµè§ˆå™¨ (ä»¥ appuser èº«ä»½) ---
# è¿™å°†ç¡®ä¿æµè§ˆå™¨è¢«ä¸‹è½½åˆ° /home/appuser/.cache/ms-playwright/ ç›®å½•ä¸‹ã€‚
RUN python -m playwright install chromium

# --- 6. æš´éœ²ç«¯å£å¹¶å®šä¹‰å¯åŠ¨å‘½ä»¤ ---
EXPOSE 8000
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "1"]


--- æ–‡ä»¶è·¯å¾„: docker-compose.yml ---

# æ–‡ä»¶è·¯å¾„: docker-compose.yml

services:
  nginx:
    image: nginx:latest
    container_name: cursor-2api-nginx
    restart: always
    ports:
      - "${NGINX_PORT:-8088}:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - app
    networks:
      - cursor-net

  app:
    build:
      context: .
      dockerfile: Dockerfile
    # container_name is removed to allow multiple instances
    restart: unless-stopped
    env_file:
      - .env
    # shm_size is crucial for Playwright in Docker
    shm_size: '1gb'
    networks:
      - cursor-net

networks:
  cursor-net:
    driver: bridge


--- æ–‡ä»¶è·¯å¾„: main.py ---

# æ–‡ä»¶è·¯å¾„: main.py

import logging
from contextlib import asynccontextmanager
from typing import Optional
import nest_asyncio

from fastapi import FastAPI, Request, HTTPException, Depends, Header
from fastapi.responses import JSONResponse

from app.core.config import settings
from app.providers.cursor_provider import CursorProvider, PlaywrightManager

nest_asyncio.apply()

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

playwright_manager = PlaywrightManager()
provider = CursorProvider(playwright_manager)

@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼Œåˆå§‹åŒ–å¹¶ç»´æŠ¤ä¸€ä¸ªæŒä¹…åŒ–çš„ã€å…·å¤‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›çš„æµè§ˆå™¨ä¼šè¯ã€‚"""
    logger.info(f"å¯åŠ¨ {settings.APP_NAME} v{settings.APP_VERSION} (åˆ›ä¸–åè®®)")
    
    await playwright_manager.start()

    logger.info("æˆ˜ç•¥æ ¸å¿ƒ: 'Playwright' å·²æ¿€æ´»ï¼ŒæŒä¹…åŒ–ä¸ªäººä¼šè¯å‡†å¤‡å°±ç»ªã€‚")
    logger.info(f"æœåŠ¡ç›‘å¬äº: http://localhost:{settings.NGINX_PORT}")
    
    yield
    
    logger.info("åº”ç”¨å…³é—­ä¸­...")
    await playwright_manager.stop()
    logger.info("Playwright å®ä¾‹å·²å…³é—­ã€‚")

app = FastAPI(
    title=settings.APP_NAME,
    version=settings.APP_VERSION,
    description="ä¸€ä¸ªåŠŸèƒ½å®Œå¤‡ã€æ”¯æŒåŠ¨æ€æ¨¡å‹å’Œä¸Šä¸‹æ–‡é€‰æ‹©çš„ cursor.com ä»£ç†ã€‚",
    lifespan=lifespan
)

async def verify_api_key(authorization: Optional[str] = Header(None)):
    if settings.API_MASTER_KEY and settings.API_MASTER_KEY not in ["1", ""]:
        if not authorization or "bearer" not in authorization.lower():
            raise HTTPException(status_code=401, detail="éœ€è¦ Bearer Token è®¤è¯ã€‚")
        token = authorization.split(" ")[-1]
        if token != settings.API_MASTER_KEY:
            raise HTTPException(status_code=403, detail="æ— æ•ˆçš„ API Keyã€‚")

@app.post("/v1/chat/completions", dependencies=[Depends(verify_api_key)])
async def chat_completions(request: Request):
    try:
        request_data = await request.json()
        return await provider.chat_completion(request_data)
    except Exception as e:
        logger.error(f"å¤„ç†èŠå¤©è¯·æ±‚æ—¶å‘ç”Ÿé¡¶å±‚é”™è¯¯: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"å†…éƒ¨æœåŠ¡å™¨é”™è¯¯: {str(e)}")

@app.get("/v1/models", dependencies=[Depends(verify_api_key)], response_class=JSONResponse)
async def list_models():
    return await provider.get_models()

@app.get("/", summary="æ ¹è·¯å¾„", include_in_schema=False)
def root():
    """æ ¹è·¯å¾„ï¼Œæä¾›æœåŠ¡çŠ¶æ€ä¿¡æ¯"""
    return {
        "message": f"æ¬¢è¿æ¥åˆ° {settings.APP_NAME} v{settings.APP_VERSION}",
        "status": "ok",
        "protocol": "Genesis Protocol Â· G (Final Ultimate Pro Max) - Dynamic & Aware"
    }


--- æ–‡ä»¶è·¯å¾„: nginx.conf ---

# æ–‡ä»¶è·¯å¾„: nginx.conf

worker_processes auto;

events {
    worker_connections 1024;
}

http {
    upstream cursor_backend {
        # No ip_hash, use default round-robin for load balancing
        server app:8000;
    }

    server {
        listen 80;
        server_name localhost;

        location / {
            proxy_pass http://cursor_backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            proxy_buffering off;
            proxy_cache off;
            proxy_set_header Connection '';
            proxy_http_version 1.1;
            chunked_transfer_encoding off;
        }
    }
}


--- æ–‡ä»¶è·¯å¾„: requirements.txt ---

fastapi
uvicorn
pydantic-settings
python-dotenv
httpx
playwright
nest-asyncio


--- æ–‡ä»¶è·¯å¾„: app\core\__init__.py ---



--- æ–‡ä»¶è·¯å¾„: app\core\config.py ---

from pydantic_settings import BaseSettings, SettingsConfigDict
from typing import List, Optional

class Settings(BaseSettings):
    model_config = SettingsConfigDict(env_file=".env", env_file_encoding='utf-8', extra="ignore")

    APP_NAME: str = "cursor-2api"
    APP_VERSION: str = "2.0.0"
    DESCRIPTION: str = "ä¸€ä¸ªå°† cursor.com è½¬æ¢ä¸ºå…¼å®¹ OpenAI æ ¼å¼ API çš„é«˜æ€§èƒ½ä»£ç† (Cookie è®¤è¯ç‰ˆ)ã€‚"

    API_MASTER_KEY: Optional[str] = None
    NGINX_PORT: int = 8088
    API_REQUEST_TIMEOUT: int = 180
    
    # æ–°å¢ Cookie é…ç½®
    CURSOR_COOKIE: Optional[str] = None

    KNOWN_MODELS: List[str] = [
        "anthropic/claude-sonnet-4.5",
        "openai/gpt-5-nano",
        "google/gemini-2.5-flash"
    ]

settings = Settings()


--- æ–‡ä»¶è·¯å¾„: app\providers\__init__.py ---



--- æ–‡ä»¶è·¯å¾„: app\providers\base_provider.py ---

from abc import ABC, abstractmethod
from typing import Dict, Any, Union
from fastapi.responses import StreamingResponse, JSONResponse

class BaseProvider(ABC):
    """
    å®šä¹‰æ‰€æœ‰ Provider å¿…é¡»éµå¾ªçš„æŠ½è±¡åŸºç±»ã€‚
    """
    @abstractmethod
    async def chat_completion(
        self,
        request_data: Dict[str, Any]
    ) -> Union[StreamingResponse, JSONResponse]:
        """å¤„ç†èŠå¤©è¡¥å…¨è¯·æ±‚"""
        pass

    @abstractmethod
    async def get_models(self) -> JSONResponse:
        """è·å–æ¨¡å‹åˆ—è¡¨"""
        pass


--- æ–‡ä»¶è·¯å¾„: app\providers\cursor_provider.py ---

# æ–‡ä»¶è·¯å¾„: app/providers/cursor_provider.py

import json
import time
import logging
import uuid
import asyncio
import re
from typing import Dict, Any, AsyncGenerator, Union, Optional

from fastapi import HTTPException
from fastapi.responses import StreamingResponse, JSONResponse

from playwright.async_api import async_playwright, Browser, Page, BrowserContext

from app.core.config import settings
from app.providers.base_provider import BaseProvider
from app.utils.sse_utils import (
    create_sse_data,
    create_chat_completion_chunk,
    create_non_stream_chat_completion,
    DONE_CHUNK
)

logger = logging.getLogger(__name__)

CONTEXT_REFRESH_THRESHOLD = 0.95

class PlaywrightManager:
    """
    ç®¡ç†ä¸€ä¸ªå”¯ä¸€çš„ã€æŒä¹…åŒ–çš„æµè§ˆå™¨ä¼šè¯ï¼Œå¹¶åœ¨å¯åŠ¨æ—¶å®Œæˆæ‰€æœ‰å¿…è¦çš„å‡½æ•°ç»‘å®šã€‚
    """
    def __init__(self):
        self.browser: Optional[Browser] = None
        self.playwright = None
        self.context: Optional[BrowserContext] = None
        self.page: Optional[Page] = None
        self.queue = asyncio.Queue() # å°†é˜Ÿåˆ—æå‡åˆ° Manager å±‚é¢

    async def start(self):
        logger.info("æ­£åœ¨å¯åŠ¨ä¸ªäººä¸“å±çš„æŒä¹…åŒ– Playwright ä¼šè¯...")
        self.playwright = await async_playwright().start()
        launch_args = [
            '--no-sandbox', '--disable-setuid-sandbox', '--disable-dev-shm-usage',
            '--disable-accelerated-2d-canvas', '--no-first-run', '--no-zygote',
            '--single-process', '--disable-gpu', '--dns-prefetch-disable'
        ]
        self.browser = await self.playwright.chromium.launch(headless=True, args=launch_args)
        
        self.context = await self.browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36",
        )
        self.page = await self.context.new_page()

        # --- æ ¸å¿ƒä¿®å¤ï¼šåœ¨å¯åŠ¨æ—¶ä¸€æ¬¡æ€§æ³¨å†Œæ‰€æœ‰é€šä¿¡å‡½æ•° ---
        logger.info("æ­£åœ¨ä¸ºæŒä¹…åŒ–é¡µé¢ç»‘å®šé€šä¿¡æ¡¥æ¢...")
        await self.page.expose_function("onStreamChunk", self.queue.put_nowait)
        await self.page.expose_function("onStreamError", lambda error: self.queue.put_nowait(Exception(f"Browser-side error: {error}")))
        await self.page.expose_function("onStreamEnd", lambda: self.queue.put_nowait(None))
        logger.info("é€šä¿¡æ¡¥æ¢ç»‘å®šå®Œæˆã€‚")
        # --- ä¿®å¤ç»“æŸ ---

        stealth_script = "Object.defineProperty(navigator, 'webdriver', { get: () => false });"
        await self.page.add_init_script(stealth_script)
        await self.page.add_init_script(path="app/providers/fetch_override.js")

        await self.page.goto("https://cursor.com/docs", wait_until="networkidle")
        try:
            await self.page.locator('button[title="Expand Chat Sidebar"]').click(timeout=5000)
        except Exception:
            pass

        logger.info("ä¸ªäººä¸“å±çš„æŒä¹…åŒ–ä¼šè¯å·²åˆå§‹åŒ–å¹¶å‡†å¤‡å°±ç»ªã€‚")

    async def stop(self):
        if self.page: await self.page.close()
        if self.context: await self.context.close()
        if self.browser: await self.browser.close()
        if self.playwright: await self.playwright.stop()
        logger.info("æŒä¹…åŒ– Playwright ä¼šè¯å·²å…³é—­ã€‚")

class CursorProvider(BaseProvider):
    """
    é‡‡ç”¨â€œç»ˆæå•ä½“â€æ¶æ„ï¼Œå…·å¤‡ä¸Šä¸‹æ–‡ç›‘æ§å’Œè‡ªåŠ¨å›æ”¶èƒ½åŠ›ã€‚
    """
    def __init__(self, playwright_manager: PlaywrightManager):
        self.pm = playwright_manager
        self._lock = asyncio.Lock()

    async def chat_completion(self, request_data: Dict[str, Any]) -> Union[StreamingResponse, JSONResponse]:
        async with self._lock:
            try:
                is_stream = request_data.get("stream", False)
                if not is_stream:
                    raise NotImplementedError("Non-streamed responses are not implemented.")

                return StreamingResponse(
                    self._execute_and_stream(request_data),
                    media_type="text/event-stream"
                )
            except Exception as e:
                logger.error(f"å¤„ç† chat_completion æ—¶å‘ç”Ÿé¡¶å±‚é”™è¯¯: {e}", exc_info=True)
                raise HTTPException(status_code=500, detail=str(e))

    async def _check_and_handle_context_limit(self, page: Page, request_id: str, model_name: str) -> AsyncGenerator[bytes, None]:
        """æ£€æŸ¥ä¸Šä¸‹æ–‡Tokenä½¿ç”¨æƒ…å†µï¼Œå¦‚æœè¶…é™åˆ™åˆ·æ–°å¹¶å‘é€é€šçŸ¥ã€‚"""
        try:
            context_element_selector = "div.flex-shrink-0.border-border.border-t"
            element = page.locator(context_element_selector).filter(has_text="ä¸Šä¸‹æ–‡")
            
            if await element.count() > 0:
                text_content = await element.inner_text()
                current_tokens, max_tokens = self._parse_context_tokens(text_content)
                
                if current_tokens is not None and max_tokens is not None and max_tokens > 0:
                    usage_ratio = current_tokens / max_tokens
                    logger.info(f"å½“å‰ä¸Šä¸‹æ–‡ä½¿ç”¨ç‡: {usage_ratio:.2%} ({current_tokens}/{max_tokens})")
                    
                    if usage_ratio >= CONTEXT_REFRESH_THRESHOLD:
                        logger.warning(f"ä¸Šä¸‹æ–‡ä½¿ç”¨ç‡è¾¾åˆ°é˜ˆå€¼ï¼Œå°†è‡ªåŠ¨åˆ·æ–°ä¼šè¯ã€‚")
                        notification_chunk = create_chat_completion_chunk(
                            request_id, model_name, "[ç³»ç»Ÿæç¤ºï¼šæ£€æµ‹åˆ°ä¸Šä¸‹æ–‡å·²æ»¡ï¼Œå·²ä¸ºæ‚¨è‡ªåŠ¨å¼€å¯æ–°å¯¹è¯ã€‚]\n\n"
                        )
                        yield create_sse_data(notification_chunk)
                        
                        await page.reload(wait_until="networkidle")
                        await page.locator('button[title="Expand Chat Sidebar"]').click(timeout=5000)
                        logger.info("é¡µé¢å·²åˆ·æ–°ï¼Œä¸Šä¸‹æ–‡å·²æ¸…ç©ºã€‚")
        except Exception as e:
            logger.error(f"æ£€æŸ¥ä¸Šä¸‹æ–‡é™åˆ¶æ—¶å‡ºé”™: {e}", exc_info=True)

    def _parse_context_tokens(self, text: str) -> (Optional[float], Optional[float]):
        match = re.search(r'ä¸Šä¸‹æ–‡ï¼š\s*([\d\.]+)k\s*/\s*([\d\.]+)k', text, re.IGNORECASE)
        if match:
            try:
                current_k = float(match.group(1))
                max_k = float(match.group(2))
                return current_k * 1000, max_k * 1000
            except (ValueError, IndexError):
                return None, None
        return None, None

    async def _execute_and_stream(self, request_data: Dict[str, Any]) -> AsyncGenerator[bytes, None]:
        page = self.pm.page
        queue = self.pm.queue # ä½¿ç”¨å…±äº«çš„é˜Ÿåˆ—
        if not page:
            raise RuntimeError("æŒä¹…åŒ–é¡µé¢æœªåˆå§‹åŒ–ã€‚")

        request_id = f"chatcmpl-{uuid.uuid4()}"
        model_name = request_data.get("model", "anthropic/claude-3.5-sonnet")

        try:
            # --- æ ¸å¿ƒä¿®å¤ï¼šä¸å†é‡å¤æ³¨å†Œå‡½æ•° ---

            async for notification in self._check_and_handle_context_limit(page, request_id, model_name):
                yield notification

            payload = self._prepare_payload(request_data)
            last_user_prompt = self._get_last_user_prompt(request_data)
            if not last_user_prompt:
                raise ValueError("è¯·æ±‚ä¸­ä¸åŒ…å«æœ‰æ•ˆçš„ç”¨æˆ·æ¶ˆæ¯ã€‚")

            await page.evaluate("(payload) => { window.chatPayload = payload; }", payload)
            
            await page.locator('textarea[placeholder*="Ask"]').fill(last_user_prompt)
            
            logger.info("ç‚¹å‡»å‘é€æŒ‰é’®ï¼Œè§¦å‘è¢«ä»£ç†çš„ fetch...")
            await page.locator('button[type="submit"]').click()

            logger.info("ç­‰å¾…å¹¶å®æ—¶è½¬æ¢æµæ•°æ®...")
            buffer = ""
            while True:
                item = await asyncio.wait_for(queue.get(), timeout=120)
                if item is None:
                    logger.info("æµç»“æŸã€‚")
                    break
                if isinstance(item, Exception):
                    raise item
                
                buffer += item
                while "\n\n" in buffer:
                    raw_event, buffer = buffer.split("\n\n", 1)
                    if raw_event.startswith("data:"):
                        try:
                            data_str = raw_event[5:].strip()
                            if not data_str or data_str == "[DONE]":
                                continue
                            
                            cursor_data = json.loads(data_str)
                            if cursor_data.get("type") == "text-delta":
                                delta_content = cursor_data.get("delta", "")
                                if delta_content:
                                    openai_chunk = create_chat_completion_chunk(request_id, model_name, delta_content)
                                    yield create_sse_data(openai_chunk)
                        except json.JSONDecodeError:
                            logger.warning(f"æ— æ³•è§£æ Cursor æµæ•°æ®å—: {raw_event}")
                            continue
        
        except Exception as e:
            logger.error(f"æµç¼–æ’å¤±è´¥: {e}", exc_info=True)
            error_chunk = create_chat_completion_chunk(request_id, model_name, f"Orchestration error: {str(e)}", "error")
            yield create_sse_data(error_chunk)

        finally:
            yield DONE_CHUNK

    def _prepare_payload(self, request_data: Dict[str, Any]) -> Dict[str, Any]:
        user_string = request_data.get("user", "")
        file_path = "/docs/"
        if "@" in user_string:
            parts = user_string.split("@", 1)
            if len(parts) == 2 and parts[1].startswith("/"):
                file_path = parts[1]
                logger.info(f"æ£€æµ‹åˆ°ä¸Šä¸‹æ–‡è·¯å¾„: {file_path}")

        context = [{"type": "file", "content": "", "filePath": file_path}]

        cursor_messages = []
        for msg in request_data.get("messages", []):
            role = msg.get("role")
            content = msg.get("content")
            
            if role and content:
                cursor_messages.append({
                    "role": role,
                    "parts": [{"type": "text", "text": content}],
                    "id": f"msg_{uuid.uuid4().hex[:16]}"
                })

        return {
            "context": context,
            "model": request_data.get("model", "anthropic/claude-3.5-sonnet"),
            "id": f"req_{uuid.uuid4().hex[:16]}",
            "messages": cursor_messages,
            "trigger": "submit-message"
        }

    def _get_last_user_prompt(self, request_data: Dict[str, Any]) -> str:
        user_messages = [msg for msg in request_data.get("messages", []) if msg.get("role") == "user"]
        if not user_messages: return ""
        
        last_message_content = user_messages[-1].get("content", "")
        if isinstance(last_message_content, list):
            return " ".join(p.get("text", "") for p in last_message_content if p.get("type") == "text")
        return last_message_content
    
    async def get_models(self) -> JSONResponse:
        model_data = {
            "object": "list",
            "data": [{"id": name, "object": "model", "created": int(time.time()), "owned_by": "lzA6"} for name in settings.KNOWN_MODELS]
        }
        return JSONResponse(content=model_data)


--- æ–‡ä»¶è·¯å¾„: app\providers\fetch_override.js ---

// æ–‡ä»¶è·¯å¾„: app/providers/fetch_override.js

(function() {
    const originalFetch = window.fetch;

    window.fetch = new Proxy(originalFetch, {
        apply: async function(target, thisArg, args) {
            let [url, config] = args;

            if (typeof url === 'string' && url.includes('/api/chat') && config && config.method === 'POST') {
                console.log('Apollo Protocol: Intercepting fetch to /api/chat.');

                // --- æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨ä» Python æ³¨å…¥çš„ã€åŒ…å«å®Œæ•´å†å²çš„ payload ---
                if (window.chatPayload) {
                    console.log('Apollo Protocol: Overriding request body with payload from Python.');
                    config.body = JSON.stringify(window.chatPayload);
                    // æ›´æ–° args[1] ä»¥ç¡®ä¿ä¿®æ”¹ç”Ÿæ•ˆ
                    args[1] = config;
                } else {
                    console.warn('Apollo Protocol: window.chatPayload not found. Using original request body.');
                }
                // --- ä¿®æ”¹ç»“æŸ ---

                try {
                    const response = await Reflect.apply(target, thisArg, args);

                    if (!response.ok) {
                        const errorText = await response.text();
                        const errorMessage = `Fetch failed via proxy: ${response.status} ${response.statusText}. Body: ${errorText}`;
                        if (window.onStreamError) window.onStreamError(errorMessage);
                        return new Response(errorText, { status: response.status, headers: response.headers });
                    }

                    if (!response.body) {
                        if (window.onStreamError) window.onStreamError('Response has no body');
                        return response;
                    }

                    const [streamForPython, streamForBrowser] = response.body.tee();

                    (async () => {
                        const reader = streamForPython.getReader();
                        const decoder = new TextDecoder();
                        try {
                            while (true) {
                                const { done, value } = await reader.read();
                                if (done) break;
                                const chunk = decoder.decode(value, { stream: true });
                                if (window.onStreamChunk) window.onStreamChunk(chunk);
                            }
                        } catch (e) {
                            if (window.onStreamError) window.onStreamError('Error reading stream: ' + e.toString());
                        } finally {
                            if (window.onStreamEnd) window.onStreamEnd();
                        }
                    })();
                    
                    return new Response(streamForBrowser, {
                        status: response.status,
                        statusText: response.statusText,
                        headers: response.headers,
                    });

                } catch (error) {
                    if (window.onStreamError) window.onStreamError(error.toString());
                    throw error;
                }
            }

            return Reflect.apply(target, thisArg, args);
        }
    });
})();


--- æ–‡ä»¶è·¯å¾„: app\utils\sse_utils.py ---

import json
import time
from typing import Dict, Any, Optional

# SSE 'data: [DONE]' ç»“æŸæ ‡å¿—
DONE_CHUNK = b"data: [DONE]\n\n"

def create_sse_data(data: Dict[str, Any]) -> bytes:
    """å°†å­—å…¸è½¬æ¢ä¸º SSE æ ¼å¼çš„å­—èŠ‚ä¸²"""
    return f"data: {json.dumps(data)}\n\n".encode('utf-8')

def create_chat_completion_chunk(
    request_id: str,
    model: str,
    content: str,
    finish_reason: Optional[str] = None
) -> Dict[str, Any]:
    """åˆ›å»º OpenAI æ ¼å¼çš„æµå¼èŠå¤©è¡¥å…¨å—"""
    return {
        "id": request_id,
        "object": "chat.completion.chunk",
        "created": int(time.time()),
        "model": model,
        "choices": [
            {
                "index": 0,
                "delta": {"content": content},
                "finish_reason": finish_reason,
                "logprobs": None
            }
        ],
    }

def create_non_stream_chat_completion(
    request_id: str,
    model: str,
    content: str,
    finish_reason: str = "stop"
) -> Dict[str, Any]:
    """åˆ›å»º OpenAI æ ¼å¼çš„éæµå¼èŠå¤©è¡¥å…¨å“åº”"""
    created_time = int(time.time())
    return {
        "id": request_id,
        "object": "chat.completion",
        "created": created_time,
        "model": model,
        "choices": [
            {
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": content,
                },
                "finish_reason": finish_reason,
            }
        ],
        "usage": {
            "prompt_tokens": -1,  # æ— æ³•ç²¾ç¡®è®¡ç®—ï¼Œè®¾ä¸º-1
            "completion_tokens": len(content),
            "total_tokens": -1,
        },
    }



